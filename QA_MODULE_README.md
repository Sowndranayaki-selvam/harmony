# ğŸ¤– Custom Q&A Module

Your PDF-based question answering system that **only uses your data** - no external knowledge or third-party data allowed! Uses **FREE, local AI** with Ollama - no API keys or billing needed!

## âœ¨ Features

- âœ… Trains on **your `full data.pdf`** only
- âœ… **No external knowledge** - answers come exclusively from your data
- âœ… **100% FREE** - Uses Ollama local LLM (no API costs!)
- âœ… Vector embeddings for fast, accurate retrieval
- âœ… Source tracking - see which pages answer your questions
- âœ… Interactive & programmatic interface
- âœ… Built with LangChain + ChromaDB + HuggingFace embeddings

## ğŸš€ Quick Start (3 Steps)

### Step 1ï¸âƒ£ Install Ollama (One-time setup)

**Windows:**
1. Download from [ollama.ai](https://ollama.ai)
2. Install the .exe
3. Run this in PowerShell:
```powershell
ollama pull mistral
```

### Step 2ï¸âƒ£ Start Ollama Server

Keep this running while using Q&A module:
```powershell
ollama serve
```

### Step 3ï¸âƒ£ Run Q&A Module

In a **new** PowerShell window:
```powershell
cd "c:\Users\USER\Desktop\harmony\python\openai_harmony"
python ask.py
```

**That's it!** Start asking questions! ğŸ¯

## ğŸ’° Cost: $0

- âœ… No API keys needed
- âœ… No billing accounts  
- âœ… No rate limits
- âœ… Runs locally on your machine

## ğŸ¯ Example Usage

```
============================================================
ğŸ¤– Q&A Module - Trained on Your PDF Data Only
============================================================

âš™ï¸  Checking for Ollama (local AI model)...
âœ… Ollama found

ğŸ”„ Initializing Q&A module...
ğŸ“„ Loading PDF: ../../data/full data.pdf
âœ… Loaded 275 text chunks from PDF

============================================================
ğŸ“ Ask questions about your PDF (type 'quit' to exit)
============================================================

â“ Your question: What is the main topic?
ğŸ” Searching your data...
âœ… Answer: Based on your PDF, the main topic is...
ğŸ“„ Source pages: [1, 3, 5]

â“ Your question: quit
ğŸ‘‹ Goodbye!
```

## ğŸ“ File Structure

```
harmony/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ full data.pdf          â† Your training data
â”‚   â””â”€â”€ chroma_db/             â† Vector embeddings (auto-created)
â”œâ”€â”€ python/openai_harmony/
â”‚   â”œâ”€â”€ qa_module.py           â† Main Q&A logic
â”‚   â”œâ”€â”€ ask.py                 â† Interactive interface
â”‚   â””â”€â”€ __init__.py            â† Module exports
â”œâ”€â”€ setup_ollama.bat           â† Auto-setup script (Windows)
â””â”€â”€ QA_MODULE_README.md        â† This file
```

## ğŸ”’ Data Privacy

- Your PDF stays on **YOUR computer**
- All processing is **local** (except AI model inference)
- No cloud storage
- No tracking
- Zero data collection

## ğŸ“– Programmatic Usage

```python
from openai_harmony import CustomQAModule

# Initialize
qa = CustomQAModule("../../data/full data.pdf")

# Ask questions
result = qa.ask("What is the main topic?")
print(result["answer"])
print(result["sources"])

# Batch questions
results = qa.batch_ask([
    "Question 1?",
    "Question 2?",
])
```

## âš™ï¸ How It Works

1. **Load PDF** â†’ Extract text from your document
2. **Split into chunks** â†’ Break into searchable pieces
3. **Create embeddings** â†’ Convert to vectors (HuggingFace - free!)
4. **Store in Vector DB** â†’ ChromaDB for fast retrieval
5. **Process questions** â†’ Find relevant chunks from YOUR PDF
6. **Generate answers** â†’ Local Ollama AI uses only your data

## ğŸ› ï¸ Troubleshooting

### "Ollama not found"
- Install from https://ollama.ai
- Make sure `ollama serve` is running in another terminal

### "Connection refused to localhost:11434"
- Start Ollama: `ollama serve`
- Keep that terminal open while using Q&A module

### "Model not found"
- Run: `ollama pull mistral`
- Wait for download to complete

### Slow responses
- First run creates embeddings (takes 1-2 min)
- Subsequent runs are faster (cached embeddings)
- Responses depend on PDF complexity

## ğŸ“¦ Installed Packages

```
PyPDF2              - PDF reading
langchain           - LLM framework
langchain-core      - Core components
langchain-huggingface - Free embeddings
chromadb            - Vector database
ollama              - Local AI models
sentence-transformers - Embedding models
```

## ğŸ“ Advanced: Use Different Models

Want a faster model or specific language?

```bash
# Faster, smaller model
ollama pull neural-chat

# Better quality, larger model  
ollama pull llama2

# Download and use
ollama pull phi
```

Then update `qa_module.py`:
```python
llm = Ollama(
    model="your-model-name",  # Change this
    temperature=0,
    base_url="http://localhost:11434"
)
```

## ğŸ“ Notes

- âœ… PDF: 275 text chunks extracted
- âœ… Embedding model: sentence-transformers/all-MiniLM-L6-v2 (free, fast)
- âœ… LLM: Mistral (7B parameters, accurate)
- âœ… Storage: Local ChromaDB (~50MB)

## ğŸš¨ Important

This module **ONLY answers from your PDF**. It will NOT:
- âŒ Use Google or internet searches
- âŒ Use external knowledge bases
- âŒ Access third-party APIs
- âŒ Use real-time data

Perfect for confidential documents! ğŸ”’

---

**Your module is ready!** ğŸ‰

Start Ollama â†’ Ask questions â†’ Get answers from YOUR data!

